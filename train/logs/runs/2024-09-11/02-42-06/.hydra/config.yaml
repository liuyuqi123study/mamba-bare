work_dir: ${hydra:runtime.cwd}
data_dir: ${work_dir}/data/
print_config: true
ignore_warnings: true
test_after_training: true
resume: true
seed: 1111
name: ${.expt_name}
trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: gpu
  min_epochs: 1
  max_epochs: 1000
  strategy: auto
  devices: 1
  num_nodes: 1
  accumulate_grad_batches: ${div_up:${train.global_batch_size}, ${eval:${trainer.devices}
    * ${datamodule.batch_size} * ${trainer.num_nodes}}}
  max_steps: 20000
  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
  check_val_every_n_epoch: null
  precision: bf16
  gradient_clip_val: 1.0
train:
  optimizer:
    _target_: apex.optimizers.FusedAdam
    adam_w_mode: true
    lr: 0.0008
    weight_decay: 0.1
    betas:
    - 0.9
    - 0.95
  scheduler:
    _target_: train.optim.timm_lr_scheduler.TimmCosineLRScheduler
    t_in_epochs: false
    t_initial: 19800
    warmup_lr_init: 1.0e-06
    warmup_t: 200
    lr_min: 8.0e-05
    warmup_prefix: true
  gpu_mem: ${eval:"round(float(__import__('subprocess').check_output('nvidia-smi -i
    0 --query-gpu=memory.total --format=csv,noheader,nounits', shell=True).strip().decode())
    / 1000)"}
  global_batch_size: 256
  optimizer_param_grouping:
    bias_weight_decay: false
    normalization_weight_decay: false
  loss_fn:
    _target_: flash_attn.losses.cross_entropy.CrossEntropyLoss
    inplace_backward: true
task:
  _target_: seq.SequenceLMModel
model:
  _target_: mambapp.MambaLMHeadModel
  _recursive_: true
  config:
    _target_: mambapp.MambaConfig
    reorder_and_upcast_attn: false
    scale_attn_by_inverse_layer_idx: true
    n_positions: ${datamodule.max_length}
    n_embd: 1024
    n_head: 16
    n_layer: 24
    residual_in_fp32: true
    use_flash_attn: true
    fused_dropout_add_ln: true
    fused_mlp: true
    fused_bias_fc: true
    pad_vocab_size_multiple: 16
    d_model: 768
    rms_norm: true
datamodule:
  _target_: train.datamodules.language_modeling_hf.LMDataModule
  dataset_name: EleutherAI/pile
  tokenizer_name: gpt2
  cache_dir: ${oc.env:DATA_DIR,${data_dir}}/wikitext103/cache
  max_length: 2048
  add_eos: true
  batch_size: 24
  batch_size_eval: 24
  num_workers: 64
  use_shmem: false
  shuffle: true
  pin_memory: true
  __train_len: ${div_up:374337375694, ${.max_length}}
  fault_tolerant: true
  ddp: ${eval:"${trainer.devices} > 1"}
callbacks:
  rich_model_summary:
    _target_: pytorch_lightning.callbacks.RichModelSummary
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val/loss
    mode: min
    save_top_k: 3
    save_last: true
    verbose: false
    dirpath: ../checkpoints/${expt_name}
    filename: step_{step}
    auto_insert_metric_name: false
    every_n_train_steps: 1000
  early_stopping: null
  learning_rate_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step
  speed_monitor:
    _target_: speed_monitor.SpeedMonitor
    intra_step_time: true
    inter_step_time: true
    epoch_time: true
  loss_scale_monitor:
    _target_: loss_scale_monitor.LossScaleMonitor
  params_log:
    _target_: params_log.ParamsLog
    total_params_log: true
    trainable_params_log: true
    non_trainable_params_log: true
  gpu_affinity:
    _target_: gpu_affinity.GpuAffinity
  norm_monitor:
    _target_: norm_monitor.NormMonitor
eval:
  metrics:
    ppl:
      _target_: perplexity.Perplexity
    num-tokens:
      _target_: num_tokens.NumTokens
  log_on_step: true
  ckpt: //content/drive/MyDrive/mamba-bare-main/train/130M_thepile_step_20000
logger:
  wandb:
    _target_: pytorch_lightning.loggers.wandb.WandbLogger
    project: mamba_bare
    name: ${name}
    save_dir: .
    mode: online
    id: ${oc.select:name}
    log_model: false
    prefix: ''
    job_type: train
    group: ''
    tags: []
default_mode: true
expt_name: 02-21-mambapp-130m-pile
