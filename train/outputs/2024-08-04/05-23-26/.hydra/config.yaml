train:
  optimizer:
    lr: 0.0003
    betas:
    - 0.9
    - 0.95
    _target_: apex.optimizers.FusedAdam
    adam_w_mode: true
    weight_decay: 0.1
  scheduler:
    lr_min: ${eval:0.1 * ${train.optimizer.lr}}
    _target_: train.optim.timm_lr_scheduler.TimmCosineLRScheduler
    warmup_t: ${eval:0.01 * ${trainer.max_steps}}
    t_initial: 600000
    t_in_epochs: false
    warmup_prefix: true
    warmup_lr_init: 1.0e-06
  gpu_mem: ${eval:"round(float(__import__('subprocess').check_output('nvidia-smi -i
    0 --query-gpu=memory.total --format=csv,noheader,nounits', shell=True).strip().decode())
    / 1000)"}
  global_batch_size: 256
  optimizer_param_grouping:
    bias_weight_decay: false
    normalization_weight_decay: false
  loss_fn:
    _target_: flash_attn.losses.cross_entropy.CrossEntropyLoss
    inplace_backward: true
trainer:
  val_check_interval: ${eval:2000 * ${.accumulate_grad_batches}}
  max_steps: 800000
  accelerator: gpu
  devices: 8
  num_nodes: 1
  accumulate_grad_batches: ${div_up:${train.global_batch_size}, ${eval:${trainer.devices}
    * ${datamodule.batch_size} * ${trainer.num_nodes}}}
  check_val_every_n_epoch: null
  precision: bf16
  gradient_clip_val: 1.0
  strategy: null
datamodule:
  batch_size: ${eval:"4 if ${train.gpu_mem} < 24 else (8 if ${train.gpu_mem} < 40
    else (16 if ${train.gpu_mem} < 80 else 32))"}
  batch_size_eval: ${.batch_size}
  max_length: 2048
  fault_tolerant: true
  ddp: ${eval:"${trainer.devices} > 1"}
expt_name: 02-21-mambapp-360m
name: ${.expt_name}
callbacks:
  model_checkpoint:
    dirpath: /var/cr01_data/sim_data/checkpoints/${oc.select:name,''}
    monitor: val/loss
    mode: min
    save_top_k: 3
    save_last: true
    every_n_train_steps: 1000
    filename: step_{step}
    auto_insert_metric_name: false
  early_stopping: null
resume: true
model:
  config:
    n_layer: 24
    rms_norm: true
    residual_in_fp32: true
    pad_vocab_size_multiple: 8
    n_embd: 1024
    n_head: 16
    use_flash_attn: true
    fused_dropout_add_ln: true
    fused_mlp: true
    fused_bias_fc: true
? ''
: content:
    drive:
      MyDrive:
        mamba-bare-main:
          train:
            configs:
              trainer:
                _target_: pytorch_lightning.Trainer
                accelerator: null
                min_epochs: 1
                max_epochs: 1000
              model:
                _target_: mamba_core.mambapp.MambaLMHeadModel
                _recursive_: true
                config:
                  _target_: mamba_core.mambapp.MambaConfig
                  reorder_and_upcast_attn: false
                  scale_attn_by_inverse_layer_idx: true
                  n_positions: ${datamodule.max_length}
              datamodule:
                _target_: train.datamodules.language_modeling_hf.LMDataModule
                dataset_name: DKYoon/SlimPajama-6B
                dataset_config_name: default
                tokenizer_name: gpt2
                cache_dir: ${oc.env:DATA_DIR,${data_dir}}/SlimPajama-6B/cache
                max_length: 2048
                add_eos: true
                batch_size: 4
                batch_size_eval: ${eval:${.batch_size} * 2}
                num_workers: 64
                use_shmem: false
                shuffle: true
                pin_memory: true
                __train_len: ${div_up:23997342716, ${.max_length}}
              callbacks:
                rich_model_summary:
                  _target_: pytorch_lightning.callbacks.RichModelSummary
                model_checkpoint:
                  _target_: pytorch_lightning.callbacks.ModelCheckpoint
                  monitor: val/acc
                  mode: max
                  save_top_k: 1
                  save_last: true
                  verbose: false
                  dirpath: ${oc.env:CHECKPOINT_DIR,checkpoints}/${oc.select:name,''}
                  filename: epoch_{epoch:03d}
                  auto_insert_metric_name: false
                early_stopping:
                  _target_: pytorch_lightning.callbacks.EarlyStopping
                  monitor: val/acc
                  mode: max
                  patience: 100
                  min_delta: 0
                learning_rate_monitor:
                  _target_: pytorch_lightning.callbacks.LearningRateMonitor
                  logging_interval: step
                speed_monitor:
                  _target_: train.callbacks.speed_monitor.SpeedMonitor
                  intra_step_time: true
                  inter_step_time: true
                  epoch_time: true
                loss_scale_monitor:
                  _target_: train.callbacks.loss_scale_monitor.LossScaleMonitor
                params_log:
                  _target_: train.callbacks.params_log.ParamsLog
                  total_params_log: true
                  trainable_params_log: true
                  non_trainable_params_log: true
                gpu_affinity:
                  _target_: train.callbacks.gpu_affinity.GpuAffinity
                norm_monitor:
                  _target_: train.callbacks.norm_monitor.NormMonitor
              logger:
                wandb:
                  _target_: pytorch_lightning.loggers.wandb.WandbLogger
                  project: mamba_bare
                  name: ${name}
                  save_dir: .
                  mode: online
                  id: ${oc.select:name}
                  log_model: false
                  prefix: ''
                  job_type: train
                  group: ''
                  tags: []
eval:
  metrics:
    ppl:
      _target_: train.metrics.perplexity.Perplexity
    num-tokens:
      _target_: train.metrics.num_tokens.NumTokens
  log_on_step: true
task:
  _target_: train.tasks.seq.SequenceLMModel
seed: 1111
experiment: example/mamba-360m-slim6B
